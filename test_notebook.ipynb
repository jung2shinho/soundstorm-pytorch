{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoundMime.com\n",
    "\n",
    "Imitates the voice of a human voice.\n",
    "\n",
    "- Soundstream for encoder/decoder.\n",
    "- SoundStorm for text-to-speech generation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T02:47:51.411678Z",
     "start_time": "2024-12-12T02:47:51.228594Z"
    }
   },
   "source": [
    "print(\"hello\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T02:48:05.874906Z",
     "start_time": "2024-12-12T02:48:05.867323Z"
    }
   },
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "curr_dir = os.getcwd()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T02:48:13.863290Z",
     "start_time": "2024-12-12T02:48:13.684670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from audiolm_pytorch import EncodecWrapper\n",
    "encodec = EncodecWrapper()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "  File \"C:\\Windows\\Temp\\ipykernel_18492\\377550591.py\", line 1, in <module>\n",
      "    from audiolm_pytorch import EncodecWrapper\n",
      "ModuleNotFoundError: No module named 'audiolm_pytorch'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\.venv\\Lib\\site-packages\\pygments\\styles\\__init__.py\", line 45, in get_style_by_name\n",
      "ModuleNotFoundError: No module named 'pygments.styles.default'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2168, in showtraceback\n",
      "  File \"C:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1457, in structured_traceback\n",
      "  File \"C:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1348, in structured_traceback\n",
      "  File \"C:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1195, in structured_traceback\n",
      "  File \"C:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1085, in format_exception_as_a_whole\n",
      "  File \"C:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1136, in get_records\n",
      "  File \"C:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\.venv\\Lib\\site-packages\\pygments\\styles\\__init__.py\", line 47, in get_style_by_name\n",
      "pygments.util.ClassNotFound: Could not find style module 'pygments.styles.default', though it should be builtin.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-11T00:05:51.922656Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msoundstorm_pytorch\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# torchaudio.set_audio_backend(\"soundfile\") \u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(torchaudio\u001B[38;5;241m.\u001B[39mlist_audio_backends())\n",
      "File \u001B[1;32mc:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\soundstorm_pytorch\\__init__.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msoundstorm_pytorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msoundstorm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      2\u001B[0m     SoundStorm,\n\u001B[0;32m      3\u001B[0m     SoundStream,\n\u001B[0;32m      4\u001B[0m     ConformerWrapper,\n\u001B[0;32m      5\u001B[0m     Conformer\n\u001B[0;32m      6\u001B[0m )\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msoundstorm_pytorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtrainer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      8\u001B[0m     SoundStormTrainer\n\u001B[0;32m      9\u001B[0m )\n",
      "File \u001B[1;32mc:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\soundstorm_pytorch\\soundstorm.py:25\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbeartype\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Any\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msoundstorm_pytorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mattend\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Attend\n\u001B[1;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mspear_tts_pytorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TextToSemantic\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maudiolm_pytorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SoundStream\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maudiolm_pytorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HubertWithKmeans, FairseqVQWav2Vec\n",
      "File \u001B[1;32mc:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\.venv\\Lib\\site-packages\\spear_tts_pytorch\\__init__.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mspear_tts_pytorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspear_tts_pytorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      2\u001B[0m     TextToSemantic,\n\u001B[0;32m      3\u001B[0m     SpeechSpeechPretrainWrapper,\n\u001B[0;32m      4\u001B[0m     SemanticToTextWrapper,\n\u001B[0;32m      5\u001B[0m     TextToSemanticWrapper,\n\u001B[0;32m      6\u001B[0m     SemanticToTextDatasetGenerator\n\u001B[0;32m      7\u001B[0m )\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mspear_tts_pytorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtrainer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     10\u001B[0m     SpeechSpeechPretrainer,\n\u001B[0;32m     11\u001B[0m     SemanticToTextTrainer,\n\u001B[0;32m     12\u001B[0m     TextToSemanticTrainer\n\u001B[0;32m     13\u001B[0m )\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mspear_tts_pytorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     16\u001B[0m     GeneratedAudioTextDataset,\n\u001B[0;32m     17\u001B[0m     MockDataset\n\u001B[0;32m     18\u001B[0m )\n",
      "File \u001B[1;32mc:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\.venv\\Lib\\site-packages\\spear_tts_pytorch\\spear_tts_pytorch.py:18\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01meinops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m rearrange, repeat, pack, reduce\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01meinops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Rearrange\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maudiolm_pytorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FairseqVQWav2Vec, HubertWithKmeans\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maudiolm_pytorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_dataloader\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mrotary_embedding_torch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RotaryEmbedding\n",
      "File \u001B[1;32mc:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\.venv\\Lib\\site-packages\\audiolm_pytorch\\__init__.py:8\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01meinops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_torch_specific\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m allow_ops_in_compiled_graph\n\u001B[0;32m      6\u001B[0m     allow_ops_in_compiled_graph()\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maudiolm_pytorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01maudiolm_pytorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AudioLM\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maudiolm_pytorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msoundstream\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SoundStream, AudioLMSoundStream, MusicLMSoundStream\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maudiolm_pytorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mencodec\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m EncodecWrapper\n",
      "File \u001B[1;32mc:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\.venv\\Lib\\site-packages\\audiolm_pytorch\\audiolm_pytorch.py:19\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01meinops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m rearrange, repeat, reduce\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01meinops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Rearrange\n\u001B[1;32m---> 19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maudiolm_pytorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvq_wav2vec\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FairseqVQWav2Vec\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maudiolm_pytorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mhubert_kmeans\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HubertWithKmeans\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maudiolm_pytorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mt5\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n",
      "File \u001B[1;32mc:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\.venv\\Lib\\site-packages\\audiolm_pytorch\\vq_wav2vec.py:7\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nn\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01meinops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m rearrange\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mfairseq\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchaudio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m resample\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maudiolm_pytorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m curtail_to_multiple\n",
      "File \u001B[1;32mc:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\.venv\\Lib\\site-packages\\fairseq\\__init__.py:40\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mfairseq\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlr_scheduler\u001B[39;00m  \u001B[38;5;66;03m# noqa\u001B[39;00m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mfairseq\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpdb\u001B[39;00m  \u001B[38;5;66;03m# noqa\u001B[39;00m\n\u001B[1;32m---> 40\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mfairseq\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mscoring\u001B[39;00m  \u001B[38;5;66;03m# noqa\u001B[39;00m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mfairseq\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtasks\u001B[39;00m  \u001B[38;5;66;03m# noqa\u001B[39;00m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mfairseq\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtoken_generation_constraints\u001B[39;00m  \u001B[38;5;66;03m# noqa\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\.venv\\Lib\\site-packages\\fairseq\\scoring\\__init__.py:34\u001B[0m\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;129m@abstractmethod\u001B[39m\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mresult_string\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[0;32m     31\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m---> 34\u001B[0m _build_scorer, register_scorer, SCORER_REGISTRY, _ \u001B[38;5;241m=\u001B[39m registry\u001B[38;5;241m.\u001B[39msetup_registry(\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--scoring\u001B[39m\u001B[38;5;124m\"\u001B[39m, default\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbleu\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     36\u001B[0m )\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbuild_scorer\u001B[39m(choice, tgt_dict):\n\u001B[0;32m     40\u001B[0m     _choice \u001B[38;5;241m=\u001B[39m choice\u001B[38;5;241m.\u001B[39m_name \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(choice, DictConfig) \u001B[38;5;28;01melse\u001B[39;00m choice\n",
      "\u001B[1;31mTypeError\u001B[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "import soundstorm_pytorch\n",
    "# torchaudio.set_audio_backend(\"soundfile\") \n",
    "print(torchaudio.list_audio_backends())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose your pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from soundstream import from_pretrained, load\n",
    "\n",
    "# Get audio file path\n",
    "curr_dir = os.getcwd()\n",
    "file_path = os.path.join(curr_dir, 'sample1.wav')\n",
    "\n",
    "# Load the waveform\n",
    "waveform = load(file_path)\n",
    "audio_codec = from_pretrained()  # downloads model from Hugging Face\n",
    "print(waveform.shape)\n",
    "\n",
    "# quantized = audio_codec(waveform, mode='encode')\n",
    "# recovered = audio_codec(quantized, mode='decode')\n",
    "\n",
    "# torchaudio.save('out.wav', recovered[0], 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(audio_codec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soundstream import load\n",
    "\n",
    "# Provide initial audio input\n",
    "audio = load(file_path)\n",
    "audio = audio.squeeze(0)\n",
    "print(f\"audio.shape: {audio.shape}\")\n",
    "\n",
    "# encoded_features = model.encoder(audio)  # Ensure audio is in the correct format\n",
    "# # Pass the features through the quantizer to get the codes\n",
    "# quantized, codes, _ = model.quantizer(encoded_features.permute(0, 2, 1))  # Adjust dimensions if needed\n",
    "\n",
    "# # quantized = audio_codec(waveform, mode='encode')\n",
    "# # recovered = audio_codec(quantized, mode='decode')\n",
    "\n",
    "# # torchaudio.save('out.wav', recovered[0], 16000)\n",
    "\n",
    "\n",
    "# # # Identify soundstream to utilize.\n",
    "# # encodec = EncodecWrapper()\n",
    "\n",
    "# # # Use encodec as soundstream\n",
    "# # soundstream = encodec \n",
    "# # # or train your own soundstream\n",
    "\n",
    "# # Save the codes to a file\n",
    "# torch.save(codes, \"codes.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load(file_path)\n",
    "print(file_path)\n",
    "print(waveform.size())\n",
    "print(sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AudioLM\n",
    "## SoundStream & Encodec\n",
    "Neural audio compression. It focuses on compressing audio data efficiently while preserving as much quality as possible at low bitrates, aimed at real-time audio streaming or storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Soundstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiolm_pytorch import SoundStream, SoundStreamTrainer\n",
    "\n",
    "soundstream = SoundStream(\n",
    "    codebook_size = 4096,\n",
    "    rq_num_quantizers = 8,\n",
    "    rq_groups = 2,                       # this paper proposes using multi-headed residual vector quantization - https://arxiv.org/abs/2305.02765\n",
    "    use_lookup_free_quantizer = True,    # whether to use residual lookup free quantization - there are now reports of successful usage of this unpublished technique\n",
    "    use_finite_scalar_quantizer = False, # whether to use residual finite scalar quantization\n",
    "    attn_window_size = 128,              # local attention receptive field at bottleneck\n",
    "    attn_depth = 2                       # 2 local attention transformer blocks - the soundstream folks were not experts with attention, so i took the liberty to add some. encodec went with lstms, but attention should be better\n",
    ")\n",
    "\n",
    "folder_path = str(os.path.join(curr_dir,'audio_samples'))\n",
    "\n",
    "trainer = SoundStreamTrainer(\n",
    "    soundstream,\n",
    "    folder = folder_path,\n",
    "    batch_size = 3,\n",
    "    grad_accum_every = 8,         # effective batch size of 24\n",
    "    data_max_length_seconds = 2,  # train on 2 second audio\n",
    "    num_train_steps = 1_000_000\n",
    ").cpu()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# after a lot of training, you can test the autoencoding as so\n",
    "soundstream.eval() # your soundstream must be in eval mode, to avoid having the residual dropout of the residual VQ necessary for training\n",
    "\n",
    "# Provide audio input\n",
    "# audio = torch.randn(10080).cpu() \n",
    "\n",
    "# Reconstruct audio using soundstream\n",
    "recons = soundstream(audio, return_recons_only = True) # (1, 10080) - 1 channel # reconstructed audio signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained Soundstream as Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiolm_pytorch import SoundStream, SoundStreamTrainer\n",
    "\n",
    "audio = audio\n",
    "torch.load(soundstream, \"results\\soundstream0.pt\")\n",
    "# codes = soundstream.tokenize(audio)\n",
    "\n",
    "# # you can now train anything with the codebook ids\n",
    "\n",
    "# recon_audio_from_codes = soundstream.decode_from_codebook_indices(codes)\n",
    "\n",
    "\n",
    "\n",
    "# # sanity check\n",
    "\n",
    "# assert torch.allclose(\n",
    "#     recon_audio_from_codes,\n",
    "#     soundstream(audio, return_recons_only = True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoundStorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from soundstorm_pytorch import SoundStorm, ConformerWrapper\n",
    "\n",
    "conformer = ConformerWrapper(\n",
    "    codebook_size = 1024,\n",
    "    num_quantizers = 16,\n",
    "    conformer = dict(\n",
    "        dim = 512,\n",
    "        depth = 2\n",
    "    ),\n",
    ")\n",
    "\n",
    "model = SoundStorm(\n",
    "    conformer,\n",
    "    steps = 18,          # 18 steps, as in original maskgit paper\n",
    "    schedule = 'cosine'  # currently the best schedule is cosine\n",
    ")\n",
    "\n",
    "# get your pre-encoded codebook ids from the soundstream from a lot of raw audio\n",
    "codes = torch.load('codes.pt') # (batch, seq, num residual VQ)\n",
    "\n",
    "# do the below in a loop for a ton of data\n",
    "loss, _ = model(codes)\n",
    "loss.backward()\n",
    "\n",
    "# model can now generate in 18 steps. ~2 seconds sounds reasonable\n",
    "\n",
    "generated = model.generate(1024, batch_size = 2) # (2, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from soundstorm_pytorch import SoundStorm, ConformerWrapper, Conformer, SoundStream\n",
    "\n",
    "conformer = ConformerWrapper(\n",
    "    codebook_size = 1024,\n",
    "    num_quantizers = 12,\n",
    "    conformer = dict(\n",
    "        dim = 512,\n",
    "        depth = 2\n",
    "    ),\n",
    ")\n",
    "\n",
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 12,\n",
    "    attn_window_size = 128,\n",
    "    attn_depth = 2\n",
    ")\n",
    "\n",
    "model = SoundStorm(\n",
    "    conformer,\n",
    "    soundstream = soundstream   # pass in the soundstream\n",
    ")\n",
    "\n",
    "# find as much audio you'd like the model to learn\n",
    "\n",
    "# audio = torch.randn(2, 10080)\n",
    "\n",
    "# course it through the model and take a gazillion tiny steps\n",
    "\n",
    "loss, _ = model(audio)\n",
    "loss.backward()\n",
    "\n",
    "# and now you can generate state-of-the-art speech\n",
    "\n",
    "generated_audio = model.generate(seconds = 30, batch_size = 2)  # generate 30 seconds of audio (it will calculate the length in seconds based off the sampling frequency and cumulative downsamples in the soundstream passed in above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_audio.shape)\n",
    "# Ensure the generated audio is in the correct shape\n",
    "# (this assumes the model generates in [batch_size, num_channels, num_frames] format)\n",
    "# If the audio is stereo, ensure the shape is [2, num_frames]\n",
    "# Save to .wav file using torchaudio\n",
    "torchaudio.save('generated_audio.wav', generated_audio, 16000)  # 16000 Hz sample rate as an example\n",
    "\n",
    "print(\"Audio generated and saved as 'generated_audio.wav'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear_tts_pytorch import TextToSemantic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "text_to_semantic = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "text_to_semantic = TextToSemantic(\n",
    "    dim = 512,\n",
    "    source_depth = 12,\n",
    "    target_depth = 12,\n",
    "    num_text_token_ids = 50000,\n",
    "    num_semantic_token_ids = 20000,\n",
    "    use_openai_tokenizer = False\n",
    ")\n",
    "\n",
    "# # load the trained text-to-semantic transformer\n",
    "\n",
    "# text_to_semantic.load(model)\n",
    "\n",
    "# pass it into the soundstorm\n",
    "\n",
    "model_tts = SoundStorm(\n",
    "    conformer,\n",
    "    soundstream = ss_model,\n",
    "    spear_tts_text_to_semantic = text_to_semantic\n",
    ").cpu()\n",
    "\n",
    "# and now you can generate state-of-the-art speech\n",
    "\n",
    "generated_speech = model_tts.generate(\n",
    "    texts = [\n",
    "        'the rain in spain stays mainly in the plain',\n",
    "        'the quick brown fox jumps over the lazy dog'\n",
    "    ],\n",
    "    seconds=30,  # specify the number of seconds of audio to generate\n",
    "    batch_size=2\n",
    ") # (2, n) - raw waveform decoded from soundstream"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
