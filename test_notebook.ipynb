{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoundMime.com\n",
    "\n",
    "Imitates the voice of a human voice.\n",
    "\n",
    "- Soundstream for encoder/decoder.\n",
    "- SoundStorm for text-to-speech generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "curr_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiolm_pytorch import EncodecWrapper\n",
    "encodec = EncodecWrapper()\n",
    "# Now you can use the encodec variable in the same way you'd use the soundstream variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['soundfile']\n"
     ]
    }
   ],
   "source": [
    "import soundstorm_pytorch\n",
    "# torchaudio.set_audio_backend(\"soundfile\") \n",
    "print(torchaudio.list_audio_backends())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waveform.shape: torch.Size([2, 4432896])\n",
      "sample_rate: 48000\n"
     ]
    }
   ],
   "source": [
    "from soundstream import from_pretrained, load\n",
    "\n",
    "# Provide initial audio input\n",
    "curr_dir = os.getcwd()\n",
    "file_path = os.path.join(curr_dir, 'audio_samples', 'sample1.wav')\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(file_path)\n",
    "print(f\"waveform.shape: {waveform.shape}\")\n",
    "print(f\"sample_rate: {sample_rate}\")\n",
    "\n",
    "waveform = load(file_path)\n",
    "audio = waveform.mean(dim=1, keepdim=False)\n",
    "\n",
    "audio_codec = from_pretrained()  # downloads model from Hugging Face\n",
    "\n",
    "# quantized = audio_codec(waveform, mode='encode')\n",
    "# recovered = audio_codec(quantized, mode='decode')\n",
    "\n",
    "# torchaudio.save('out.wav', recovered[0], 16000)\n",
    "\n",
    "\n",
    "# # Identify soundstream to utilize.\n",
    "# encodec = EncodecWrapper()\n",
    "\n",
    "# # Use encodec as soundstream\n",
    "# soundstream = encodec \n",
    "# # or train your own soundstream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoundStream  \n",
    "Neural audio compression. It focuses on compressing audio data efficiently while preserving as much quality as possible at low bitrates, aimed at real-time audio streaming or storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Soundstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudiolm_pytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SoundStream, SoundStreamTrainer\n\u001b[0;32m      3\u001b[0m soundstream \u001b[38;5;241m=\u001b[39m SoundStream(\n\u001b[0;32m      4\u001b[0m     codebook_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m,\n\u001b[0;32m      5\u001b[0m     rq_num_quantizers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     attn_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m                       \u001b[38;5;66;03m# 2 local attention transformer blocks - the soundstream folks were not experts with attention, so i took the liberty to add some. encodec went with lstms, but attention should be better\u001b[39;00m\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(curr_dir,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_samples\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\.venv\\Lib\\site-packages\\audiolm_pytorch\\__init__.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_torch_specific\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m allow_ops_in_compiled_graph\n\u001b[0;32m      6\u001b[0m     allow_ops_in_compiled_graph()\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudiolm_pytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudiolm_pytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioLM\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudiolm_pytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msoundstream\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SoundStream, AudioLMSoundStream, MusicLMSoundStream\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudiolm_pytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mencodec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EncodecWrapper\n",
      "File \u001b[1;32mc:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\.venv\\Lib\\site-packages\\audiolm_pytorch\\audiolm_pytorch.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rearrange, repeat, reduce\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rearrange\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudiolm_pytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvq_wav2vec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FairseqVQWav2Vec\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudiolm_pytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhubert_kmeans\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HubertWithKmeans\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudiolm_pytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mt5\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n",
      "File \u001b[1;32mc:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\.venv\\Lib\\site-packages\\audiolm_pytorch\\vq_wav2vec.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rearrange\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resample\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudiolm_pytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m curtail_to_multiple\n",
      "File \u001b[1;32mc:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\.venv\\Lib\\site-packages\\fairseq\\__init__.py:40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlr_scheduler\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdb\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscoring\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtoken_generation_constraints\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jung2\\OneDrive\\G Drive\\My Projects\\Coding Projects\\soundstorm-pytorch\\.venv\\Lib\\site-packages\\fairseq\\scoring\\__init__.py:34\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;129m@abstractmethod\u001b[39m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult_string\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m _build_scorer, register_scorer, SCORER_REGISTRY, _ \u001b[38;5;241m=\u001b[39m registry\u001b[38;5;241m.\u001b[39msetup_registry(\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--scoring\u001b[39m\u001b[38;5;124m\"\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbleu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_scorer\u001b[39m(choice, tgt_dict):\n\u001b[0;32m     40\u001b[0m     _choice \u001b[38;5;241m=\u001b[39m choice\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(choice, DictConfig) \u001b[38;5;28;01melse\u001b[39;00m choice\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "from audiolm_pytorch import SoundStream, SoundStreamTrainer\n",
    "\n",
    "soundstream = SoundStream(\n",
    "    codebook_size = 4096,\n",
    "    rq_num_quantizers = 8,\n",
    "    rq_groups = 2,                       # this paper proposes using multi-headed residual vector quantization - https://arxiv.org/abs/2305.02765\n",
    "    use_lookup_free_quantizer = True,    # whether to use residual lookup free quantization - there are now reports of successful usage of this unpublished technique\n",
    "    use_finite_scalar_quantizer = False, # whether to use residual finite scalar quantization\n",
    "    attn_window_size = 128,              # local attention receptive field at bottleneck\n",
    "    attn_depth = 2                       # 2 local attention transformer blocks - the soundstream folks were not experts with attention, so i took the liberty to add some. encodec went with lstms, but attention should be better\n",
    ")\n",
    "\n",
    "folder_path = str(os.path.join(curr_dir,'audio_samples'))\n",
    "\n",
    "trainer = SoundStreamTrainer(\n",
    "    soundstream,\n",
    "    folder = folder_path,\n",
    "    batch_size = 3,\n",
    "    grad_accum_every = 8,         # effective batch size of 24\n",
    "    data_max_length_seconds = 2,  # train on 2 second audio\n",
    "    num_train_steps = 1_000_000\n",
    ").cpu()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# after a lot of training, you can test the autoencoding as so\n",
    "soundstream.eval() # your soundstream must be in eval mode, to avoid having the residual dropout of the residual VQ necessary for training\n",
    "\n",
    "# Provide audio input\n",
    "# audio = torch.randn(10080).cpu() \n",
    "\n",
    "# Reconstruct audio using soundstream\n",
    "recons = soundstream(audio, return_recons_only = True) # (1, 10080) - 1 channel # reconstructed audio signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained Soundstream as Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soundstream' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Your trained Soundstream can then be used as a generic tokenizer for audio\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# audio = torch.randn(1, 512 * 320) # input\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m codes \u001b[38;5;241m=\u001b[39m \u001b[43msoundstream\u001b[49m\u001b[38;5;241m.\u001b[39mtokenize(audio) \u001b[38;5;66;03m# convert into codes\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# you can now train anything with the codebook ids\u001b[39;00m\n\u001b[0;32m      6\u001b[0m recon_audio_from_codes \u001b[38;5;241m=\u001b[39m soundstream\u001b[38;5;241m.\u001b[39mdecode_from_codebook_indices(codes)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'soundstream' is not defined"
     ]
    }
   ],
   "source": [
    "# Your trained Soundstream can then be used as a generic tokenizer for audio\n",
    "# audio = torch.randn(1, 512 * 320) # input\n",
    "codes = soundstream.tokenize(audio) # convert into codes\n",
    "\n",
    "# you can now train anything with the codebook ids\n",
    "recon_audio_from_codes = soundstream.decode_from_codebook_indices(codes)\n",
    "\n",
    "# Sanity Check\n",
    "# Compares the reconstructed audio from codes and audio\n",
    "assert torch.allclose(\n",
    "    recon_audio_from_codes,\n",
    "    soundstream(audio, return_recons_only = True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.set_audio_backend(\"soundfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoundStorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:47<00:00,  2.63s/it]\n",
      "  6%|▌         | 1/18 [00:02<00:49,  2.91s/it]\n",
      "  6%|▌         | 1/18 [00:02<00:49,  2.91s/it]\n",
      "  6%|▌         | 1/18 [00:02<00:47,  2.78s/it]\n",
      "  6%|▌         | 1/18 [00:02<00:49,  2.89s/it]\n",
      "  6%|▌         | 1/18 [00:02<00:49,  2.89s/it]\n",
      "  6%|▌         | 1/18 [00:03<00:54,  3.18s/it]\n",
      "  6%|▌         | 1/18 [00:02<00:50,  2.96s/it]\n",
      "  6%|▌         | 1/18 [00:03<00:53,  3.12s/it]\n",
      "  6%|▌         | 1/18 [00:02<00:50,  2.94s/it]\n",
      "  6%|▌         | 1/18 [00:03<00:52,  3.09s/it]\n",
      "  6%|▌         | 1/18 [00:03<00:52,  3.09s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from soundstorm_pytorch import SoundStorm, ConformerWrapper\n",
    "\n",
    "conformer = ConformerWrapper(\n",
    "    codebook_size = 1024,\n",
    "    num_quantizers = 12,\n",
    "    conformer = dict(\n",
    "        dim = 512,\n",
    "        depth = 2\n",
    "    ),\n",
    ")\n",
    "\n",
    "model = SoundStorm(\n",
    "    conformer,\n",
    "    steps = 18,          # 18 steps, as in original maskgit paper\n",
    "    schedule = 'cosine'  # currently the best schedule is cosine\n",
    ")\n",
    "\n",
    "# get your pre-encoded codebook ids from the soundstream from a lot of raw audio\n",
    "codes = torch.randint(0, 1024, (2, 1024, 12)) # (batch, seq, num residual VQ)\n",
    "\n",
    "# do the below in a loop for a ton of data\n",
    "\n",
    "loss, _ = model(codes)\n",
    "loss.backward()\n",
    "\n",
    "# model can now generate in 18 steps. ~2 seconds sounds reasonable\n",
    "\n",
    "generated = model.generate(1024, batch_size = 2) # (2, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'soundstream_pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msoundstorm_pytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SoundStorm, ConformerWrapper\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msoundstream_pytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SoundStream\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize SoundStream Codec\u001b[39;00m\n\u001b[0;32m      6\u001b[0m soundstream \u001b[38;5;241m=\u001b[39m SoundStream(\n\u001b[0;32m      7\u001b[0m     codebook_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,  \u001b[38;5;66;03m# Must match SoundStorm's codebook size\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     num_quantizers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m,   \u001b[38;5;66;03m# Must match SoundStorm's num_quantizers\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     use_residual\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     13\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'soundstream_pytorch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from soundstorm_pytorch import SoundStorm, ConformerWrapper\n",
    "from soundstream_pytorch import SoundStream\n",
    "\n",
    "# Initialize SoundStream Codec\n",
    "soundstream = SoundStream(\n",
    "    codebook_size=1024,  # Must match SoundStorm's codebook size\n",
    "    num_quantizers=12,   # Must match SoundStorm's num_quantizers\n",
    "    channels=32,\n",
    "    strides=(2, 4, 5),   # Compression strides\n",
    "    multipliers=(2, 4, 8),\n",
    "    use_residual=True\n",
    ")\n",
    "\n",
    "# Initialize SoundStorm with the same parameters\n",
    "conformer = ConformerWrapper(\n",
    "    codebook_size=1024,\n",
    "    num_quantizers=12,\n",
    "    conformer=dict(\n",
    "        dim=512,\n",
    "        depth=2\n",
    "    ),\n",
    ")\n",
    "\n",
    "model = SoundStorm(\n",
    "    conformer,\n",
    "    steps=18,           # 18 steps, as in original maskgit paper\n",
    "    schedule='cosine'   # currently the best schedule is cosine\n",
    ")\n",
    "\n",
    "# ====== Training Step ======\n",
    "\n",
    "# Example raw audio data (batch of 2 mono audio signals, 16000 samples each)\n",
    "raw_audio = torch.randn(2, 16000)  # Replace with actual audio data\n",
    "\n",
    "# Step 1: Encode raw audio to quantized codes\n",
    "codes = soundstream.encode(raw_audio)  # Shape: (batch, seq, num_quantizers)\n",
    "\n",
    "# Step 2: Train SoundStorm with these codes\n",
    "loss, _ = model(codes)\n",
    "loss.backward()\n",
    "\n",
    "# ====== Generation Step ======\n",
    "\n",
    "# Step 3: Generate new codes using SoundStorm\n",
    "generated_codes = model.generate(1024, batch_size=2)  # Shape: (2, 1024)\n",
    "\n",
    "# Step 4: Decode generated codes back to audio\n",
    "generated_audio = soundstream.decode(generated_codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "torch.randn(2, 16000)\n",
    "print(type(torch.randn(2,16000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [01:54<00:00,  6.36s/it]\n",
      "  6%|▌         | 1/18 [00:05<01:39,  5.82s/it]\n",
      "  6%|▌         | 1/18 [00:06<01:51,  6.56s/it]\n",
      "  6%|▌         | 1/18 [00:05<01:36,  5.68s/it]\n",
      "  6%|▌         | 1/18 [00:06<01:50,  6.48s/it]\n",
      "  6%|▌         | 1/18 [00:05<01:38,  5.80s/it]\n",
      "  6%|▌         | 1/18 [00:06<01:42,  6.00s/it]\n",
      "  6%|▌         | 1/18 [00:05<01:27,  5.15s/it]\n",
      "  6%|▌         | 1/18 [00:05<01:34,  5.59s/it]\n",
      "  6%|▌         | 1/18 [00:05<01:37,  5.72s/it]\n",
      "  6%|▌         | 1/18 [00:05<01:39,  5.83s/it]\n",
      "  6%|▌         | 1/18 [00:06<01:43,  6.10s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from soundstorm_pytorch import SoundStorm, ConformerWrapper, Conformer, SoundStream\n",
    "\n",
    "conformer = ConformerWrapper(\n",
    "    codebook_size = 1024,\n",
    "    num_quantizers = 12,\n",
    "    conformer = dict(\n",
    "        dim = 512,\n",
    "        depth = 2\n",
    "    ),\n",
    ")\n",
    "\n",
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 12,\n",
    "    attn_window_size = 128,\n",
    "    attn_depth = 2\n",
    ")\n",
    "\n",
    "model = SoundStorm(\n",
    "    conformer,\n",
    "    soundstream = soundstream   # pass in the soundstream\n",
    ")\n",
    "\n",
    "# find as much audio you'd like the model to learn\n",
    "\n",
    "audio = torch.randn(2, 10080)\n",
    "\n",
    "# course it through the model and take a gazillion tiny steps\n",
    "\n",
    "loss, _ = model(audio)\n",
    "loss.backward()\n",
    "\n",
    "# and now you can generate state-of-the-art speech\n",
    "\n",
    "generated_audio = model.generate(seconds = 30, batch_size = 2)  # generate 30 seconds of audio (it will calculate the length in seconds based off the sampling frequency and cumulative downsamples in the soundstream passed in above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [01:39<00:00,  5.55s/it]\n",
      "  6%|▌         | 1/18 [00:05<01:31,  5.40s/it]\n",
      "  6%|▌         | 1/18 [00:05<01:34,  5.54s/it]\n",
      "  6%|▌         | 1/18 [00:04<01:23,  4.89s/it]\n",
      "  6%|▌         | 1/18 [00:05<01:26,  5.10s/it]\n",
      "  6%|▌         | 1/18 [00:04<01:21,  4.77s/it]\n",
      "  6%|▌         | 1/18 [00:05<01:26,  5.11s/it]\n",
      "  6%|▌         | 1/18 [00:05<01:25,  5.05s/it]\n",
      "  6%|▌         | 1/18 [00:04<01:22,  4.88s/it]\n",
      "  6%|▌         | 1/18 [00:04<01:17,  4.56s/it]\n",
      "  6%|▌         | 1/18 [00:04<01:13,  4.30s/it]\n",
      "  6%|▌         | 1/18 [00:05<01:29,  5.26s/it]\n"
     ]
    }
   ],
   "source": [
    "from spear_tts_pytorch import TextToSemantic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "text_to_semantic = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "text_to_semantic = TextToSemantic(\n",
    "    dim = 512,\n",
    "    source_depth = 12,\n",
    "    target_depth = 12,\n",
    "    num_text_token_ids = 50000,\n",
    "    num_semantic_token_ids = 20000,\n",
    "    use_openai_tokenizer = False\n",
    ")\n",
    "\n",
    "# # load the trained text-to-semantic transformer\n",
    "\n",
    "# text_to_semantic.load(model)\n",
    "\n",
    "# pass it into the soundstorm\n",
    "\n",
    "model = SoundStorm(\n",
    "    conformer,\n",
    "    soundstream = soundstream,\n",
    "    spear_tts_text_to_semantic = text_to_semantic\n",
    ").cpu()\n",
    "\n",
    "# and now you can generate state-of-the-art speech\n",
    "\n",
    "generated_speech = model.generate(\n",
    "    texts = [\n",
    "        'the rain in spain stays mainly in the plain',\n",
    "        'the quick brown fox jumps over the lazy dog'\n",
    "    ],\n",
    "    seconds=30,  # specify the number of seconds of audio to generate\n",
    "    batch_size=2\n",
    ") # (2, n) - raw waveform decoded from soundstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 480000])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_speech.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the tensor to a NumPy array\n",
    "audio_np = generated_speech.numpy()\n",
    "# Define the sample rate (e.g., 44100 Hz)\n",
    "sample_rate = 16000\n",
    "\n",
    "# Save the audio file (in stereo)\n",
    "sf.write('output_audio.wav', audio_np.T, sample_rate)  # `.T` transposes to [samples, channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(generated_speech.shape) == 1:  # If it's a 1D tensor (mono)\n",
    "    generated_speech = generated_speech.unsqueeze(0)  # Add a channel dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.set_audio_backend(\"soundfile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'soundfile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msoundfile\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Ensure correct backend\u001b[39;00m\n\u001b[0;32m      5\u001b[0m torchaudio\u001b[38;5;241m.\u001b[39mset_audio_backend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msox_io\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'soundfile'"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import soundfile as sf\n",
    "\n",
    "# Ensure correct backend\n",
    "torchaudio.set_audio_backend(\"sox_io\")\n",
    "\n",
    "# Prepare the tensor (ensure it's 2D for torchaudio)\n",
    "if len(generated_speech.shape) == 1:  # If it's a 1D tensor (mono)\n",
    "    generated_speech = generated_speech.unsqueeze(0)  # Add a channel dimension\n",
    "\n",
    "# Normalize (if needed)\n",
    "generated_speech = generated_speech / torch.max(torch.abs(generated_speech))\n",
    "\n",
    "# Define the output file path\n",
    "output_file = \"generated_speech.wav\"\n",
    "\n",
    "try:\n",
    "    # Save the tensor as a WAV file using torchaudio\n",
    "    torchaudio.save(output_file, generated_speech, 22050)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error with torchaudio save: {e}\")\n",
    "    # If torchaudio fails, use soundfile as a fallback\n",
    "    generated_speech_np = generated_speech.numpy()\n",
    "    sf.write(output_file, generated_speech_np.T, 22050)  # Transpose if necessary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
